---
title: "AIM Vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{aim_vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(keras)
library(aim)
```

# Introduction

The `aim` package provides a set of tools to evaluate and visualise the importance of input features and encoded dimensions in autoencoders. This vignette demonstrates the key functionalities of the package, using the MNIST dataset as an example. We showcase how to compute input pixel importance, analyse latent dimension importance, and visualise the results through heatmaps and barplots.


# Data and Model
In this vignette we will go through the main functions in `aim` that are used for permuting both the input pixels and latent dimensions. In the example below, we use the MNIST data obtained from the `keras`. For illustrative purposes we filter the data to only include the digit 7.
To begin, we load and preprocess the data:

```{r, load_data}
# set seed
set.seed(1701)
np <- reticulate::import("numpy")
np$random$seed(1701L)
reticulate::py_run_string("import random; random.seed(1701)")
tensorflow::tf$random$set_seed(1701)

# Enable deterministic operations in TensorFlow to avoid non-deterministic behavior.
tensorflow::tf$config$experimental$enable_op_determinism()

# Load Data
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

# select only digit 7
considered_digits <- c(7)
train_indices <- which(y_train %in% considered_digits)
x_train <- x_train[train_indices, , ]
y_train <- y_train[train_indices]

test_indices <- which(y_test %in% considered_digits)
x_test <- x_test[test_indices, , ]
y_test <- y_test[test_indices]

# Preprocess Data
x_train <- array_reshape(x_train / 255, c(nrow(x_train), 784))
x_test <- array_reshape(x_test / 255, c(nrow(x_test), 784))
```

Now with our data preprocessed, we can build our autoencoder model. For this example we set the number of encoding dimensions to be XX and the number of epochs to be YY. 

```{r, ae}
# Define Autoencoder Model
encoding_dim <- 12
input_img <- layer_input(shape = c(784))

encoded <- input_img %>%
  layer_dense(units = encoding_dim, activation = 'relu')

decoded <- encoded %>%
  layer_dense(units = 784, activation = 'sigmoid')

autoencoder <- keras_model(inputs = input_img, outputs = decoded)

# Compile Model
autoencoder %>% compile(
  optimizer = 'adam',
  loss = 'binary_crossentropy'
)

# fit autoencoder
autoencoder %>% fit(
  x_train, x_train,
  epochs = 50,
  batch_size = 256,
  validation_data = list(x_test, x_test),
  verbose = FALSE
)
```

We can now view the original input images against the reconstructed images to make sure our autoencoder is learning the representations well.

```{r, in_out, fig.width = 7, fig.height = 5, fig.align = "center", fig.cap = "Comparison of input images (top row) and reconstructed images (bottom row) from the autoencoder."}
# Create Encoder Model
encoder <- keras_model(inputs = autoencoder$input, outputs = encoded)
encoded_imgs <- predict(encoder, x_test)
decoded_imgs <- predict(autoencoder, x_test)

# Plot Original and Reconstructed Images
par(mfrow = c(2, 10), mar = c(0.5, 0.5, 1, 0.5), oma = c(2, 2, 2, 2))

# Top row: Original images
for (i in 1:10) {
  img <- matrix(x_test[i, ], ncol = 28, byrow = TRUE)
  img <- t(apply(img, 2, rev))
  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n')
}

# Bottom row: Reconstructed images
for (i in 1:10) {
  img <- matrix(decoded_imgs[i, ], ncol = 28, byrow = TRUE)
  img <- t(apply(img, 2, rev))
  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n')
}
```



# Input Pixel Importance

We calculate the importance of input pixels in the latent space by shuffling the values of each pixel in the input data and monitoring the effect on the AE's performance. In this case we examine the increase in MSE after permuting the input pixels.
This enables us to assess the importance of each pixel in the learned representation. In the code below we use the `vimp_input()` function to obtain the raw pixel importance. This function takes four arguemtns, namely: 

• `encoder`: A Keras encoder model.

• `test_data`: A matrix or data frame containing the test data.

• `num_permutations`: Integer. The number of times to permute each feature. Default is 4.

• `metric` Character. The error metric to use. Default is "MSE" (Mean Squared Error).

```{r, input_vimp}
pixel_vimp <- vimp_input(encoder = encoder, test_data = x_test, num_permutations = 1, metric = 'MSE')
```

We can now plot the heatmaps generated from our input pixel importance results via the `plot_input()` function:

```{r, plot_input_dir,fig.width = 8, fig.height = 6, fig.align = "center", fig.cap = "Heatmap of input pixel importance."}
plot_input(input_vimp = pixel_vimp,
           flip_horizontal = F,
           flip_vertical = T,
           rotate = 0)
```

However, as we can see, the colour scale goes from white (low) to red (high) and looks quite noisy. We want to be able to distinguish the important patterns learned by the autoencoder. To do this we fit a regression model for each pixel against the encoded dimension to determine the directional importance by examining the sign of the slope. To do this we use the. `vimp_direction()` function. This function takes three arguments, namely: 

• `encoder`: A Keras encoder model.

• `test_data`: A matrix or data frame of test data to be encoded.

• `threshold`: Numeric. If provided, coefficients with absolute value below this threshold will be set to 0. Default is NULL.

```{r, vim_dir}
dir_vimp <- vimp_direction(encoder = encoder, test_data = x_test, threshold = NULL)
```

Before we plot the directional importance, we can also calculate the importance of the latent dimensions on the reconstruction image. To do this we use the `vimp_encoded()` function. This function takes seven arguments, namely: 

• `encoder`: A Keras encoder model.

• `test_data`: A matrix or array of test data.

• `test_labels`: A vector of test labels.

• `autoencoder`: A Keras autoencoder model.

• `classes`: A vector of class labels to calculate importance for.

• `num_permutations`: Integer. Number of permutations for importance calculation. Default is 4.

• `errorMetric`: Character. The error metric to use. Default is "MSE".

```{r, vimp_lat}
latent_vimp <- vimp_encoded(encoder = encoder,
                            test_data =  x_test,
                            test_labels = y_test,
                            autoencoder = autoencoder,
                            classes = c(7),
                            num_permutations = 1)
```

We can visualise the importance as bar plots via the `plot_encoded_vimp()` function: 

```{r, plot_lat_dir, fig.width = 8, fig.height = 6, fig.align = "center", fig.cap = "Bar plot of latent dimension importance."}
plot_encoded_vimp(encoded_data = latent_vimp, flip = TRUE)
```


Now, to visualise both the latent dimension importance and directional importance together, we can use the `plot_input_direction()`. This function takes the output from the `vimp_input()`, `vimp_direction()`, and `vimp_encoded()` functions and plots the results together. The arguments for this function are: 

• `input_vimp`: A list containing input variable importance data. Created via the \code{vimp_input} function.

• `encoded_vimp`: A data frame with encoded dimension variable importance data. Created via the \code{vimp_encoded} function.

• `direction_vimp`: A matrix with linear model summary data. Created via the \code{vimp_direction} function.

• `class`: The specific class or classes to analyse.

• `topX`: Integer. If provided, limits the visualisation to the top X most important dimensions.

• `sort`: Logical. If TRUE, sorts the importance values. Default is FALSE.

• `filter_zero`: Logical. If TRUE, filters out zero importance values (i.e., unused dimensions). Default is FALSE.

• `flip_horizontal`: Logical. If TRUE, flips the heatmaps horizontally. Default is FALSE.

• `flip_vertical`: Logical. If TRUE, flips the heatmaps vertically. Default is FALSE.

• `rotate`: Integer. Specifies the rotation of the heatmaps. 0 = no rotation, 1 = 90 degrees, 2 = 180 degrees, 3 = 270 degrees. Default is 0.

• `show_legend`: Logical. If TRUE, then show the legend.


```{r, plot, fig.width = 10, fig.height = 8, fig.align = "center", fig.cap = "Combined visualisation of input pixel importance, latent dimension importance, and directional importance."}
plot_input_direction(input_vimp = pixel_vimp,
                     encoded_vimp = latent_vimp,
                     direction_vimp = dir_vimp,
                     class = 7,
                     sort = T,
                     topX = 10,
                     flip_horizontal = F,
                     flip_vertical = T,
                     rotate = 0,
                     filter_zero = F)
```



As we can see, the heatmaps show the importnat pixels in red with the negative space mainly in blue. This highlights the important pixels and the shape of the number 7 emerges. 

## Shiny app

A shiny app is also available via the `shiny_vimp()` function. It requires three arguments, namely: 

• `input_vimp`: A list or data frame containing input variable importance data. Created via the \code{vimp_input} function.
• `encoded_vimp`: A list or data frame containing encoded variable importance data. Created via the \code{vimp_encoded} function.
• `direction_vimp`: A list or data frame containing direction variable importance data. Created via the \code{vimp_direction} function.

```{r, shiny, eval=F}
shiny_vimp(input_vimp = pixel_vimp,
                     encoded_vimp = latent_vimp,
                     direction_vimp = dir_vimp)
```

Below is a screenshot of the Shiny app in use:

```{r, shiny_screenshot, echo=FALSE, out.width='100%', fig.align='center', fig.cap="Screenshot of the Shiny app displaying input pixel and encoded dimension importance."}
knitr::include_graphics("/Users/alaninglis/Desktop/AIM/vignettes/shiny_app_screenshot.png")
```

Here we can see we have selected class `7` and are displaying the top 5 most important latent dimensions. The heatmaps are sorted by the important latent dimensions via the sort by importance option. As the orientation was incorrect, we fix this by applying the flip vertical option.







